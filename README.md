# ğŸ§  Pattern Recognition and Machine Learning Assignment (2024â€“2025)

ğŸ“š *Course:* Pattern Recognition and Machine Learning  
ğŸ›ï¸ *Faculty:* AUTh - School of Electrical and Computer Engineering  
ğŸ“… *Semester:* 7th Semester, 2024â€“2025

---

## ğŸ” Overview
This repository contains our implementation for the **Pattern Recognition & Machine Learning** assignment for 2024. The project is divided into four parts (**A, B, C, and D**), covering classification methods, probabilistic models, and machine learning techniques.


## ğŸ“ Repository Structure
```
â”œâ”€â”€ Team8-AC.ipynb         # Jupyter Notebook for Parts A, B, and C
â”œâ”€â”€ Team8-D.ipynb          # Jupyter Notebook for Part D
â”œâ”€â”€ PR_Assignment_2024.pdf # Assignment description and requirements
â”œâ”€â”€ labels8.npy            # Predicted labels from Part D
â””â”€â”€ README.md              # This documentation
```

---

## ğŸ§ª Assignment Breakdown

### ğŸ…°ï¸ Part A: Maximum Likelihood Classifier
- **ğŸ¯ Objective:** Estimate parameters \(\theta_1\) and \(\theta_2\) using Maximum Likelihood Estimation (MLE).
- **âš™ï¸ Implementation:**
  - Compute and visualize log-likelihood functions.
  - Implement classifier function \( g(x) \).
  - Evaluate classification accuracy and interpret results.

### ğŸ…±ï¸ Part B: Bayesian Estimation Classifier
- **ğŸ¯ Objective:** Improve parameter estimation using Bayesian methods.
- **âš™ï¸ Implementation:**
  - Compute posterior distributions.
  - Visualize posterior densities.
  - Implement Bayesian decision function \( h(x) \).
  - Compare to MLE performance.

### ğŸŒ³ Part C: Decision Tree & Random Forest (Iris Dataset)
- **ğŸ¯ Objective:** Build interpretable models for the Iris dataset.
- **âš™ï¸ Implementation:**
  - Use `DecisionTreeClassifier` from `sklearn` with varying depths.
  - Implement and tune `RandomForestClassifier`.
  - Visualize decision boundaries and analyze model strengths.

### ğŸ§© Part D: Custom Classification Model for Large Dataset
- **ğŸ¯ Objective:** Classify data from `datasetTV.csv` and evaluate generalization on `datasetTest.csv`.

#### ğŸ§¼ Preprocessing & Feature Engineering:
- Applied normalization and data cleaning.
- Reduced feature set to optimize performance.

#### ğŸ¤– Model Development:
- Implemented and evaluated:
  - âœ… **Support Vector Machine (SVM)** â€” *from scratch*
    - Custom implementation using hinge loss and gradient descent.
    - Supported linear and RBF kernels.
  - ğŸŒ² **Random Forest** â€” *via `sklearn`*
    - Tuned estimators, depth, and sampling strategies.
  - ğŸ” Also explored KNN and Logistic Regression for benchmarking.

#### ğŸ›ï¸ Hyperparameter Tuning:
- Used **RandomizedSearchCV** to explore parameters efficiently.
  - Tuned: `C`, `kernel`, `gamma` (SVM) and `n_estimators`, `max_depth` (RF).
- 5-fold cross-validation for robust selection.

#### ğŸ“Š Training & Evaluation:
- Trained over 25 epochs with 3 trials per config.
- Metrics used: **accuracy, precision, recall, F1-score**.
- Final accuracy: **ğŸŸ¢ 84%**
  - F1 > 0.90 for classes 0 and 2.
  - Lower scores for classes 1 and 4.

#### âš ï¸ Challenges:
- Class overlap and imbalance led to misclassifications (e.g., class 3 â†” 4).
- Confusion matrix revealed class-specific difficulties.

#### ğŸ’¾ Output:
- Final predictions saved as `labels8.npy`.

#### âœ… Conclusion:
- Strong generalization and performance.
- Improvement opportunities include advanced feature engineering and class balancing.

---

## ğŸ’¡ Suggestions for Further Improvements
- ğŸ”„ **Class Balancing Techniques:** Use SMOTE, class weighting, or data augmentation to address imbalance.
- ğŸ§  **Ensemble Strategies:** Combine models using stacking or voting for more robust predictions.
- ğŸ“ˆ **Learning Curves:** Add training/validation curves to monitor overfitting.
- ğŸ¯ **Explainability Tools:** Use SHAP or LIME for model interpretability.
- ğŸŒ **Deployability:** Wrap the model in a Flask API or Streamlit app for interactive testing.

---
